{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "import io\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"offenseval-trial.txt\", \"r\", encoding = 'utf-8') as f:\n",
    "    data = f.readlines()\n",
    "temp = [line.split(\"\\t\") for line in data]\n",
    "data = [line[0] for line in temp]\n",
    "off_label = [line[1] for line in temp]\n",
    "cat_label = [line[2] for line in temp]\n",
    "tar_label = [line[3] for line in temp]\n",
    "data_size = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75940625\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for itr in range (0,10000):\n",
    "    temp_zip = list(zip(data, off_label))\n",
    "    random.shuffle(temp_zip)\n",
    "    shuffled_data, shuffled_label = zip(*temp_zip)\n",
    "    div = int(data_size*0.9)\n",
    "    train_data = shuffled_data[0:div]\n",
    "    train_label = shuffled_label[0:div]\n",
    "    val_data = shuffled_data[div:]\n",
    "    val_label = shuffled_label[div:]\n",
    "    off_count = 0\n",
    "    not_count = 0\n",
    "    for i in range(0,len(train_label)):\n",
    "        if(train_label == \"OFF\"):\n",
    "            off_count+=1\n",
    "        else:\n",
    "            not_count+=1\n",
    "    predict = \"\"\n",
    "    for i in range(0,len(val_label)):\n",
    "        if(random.random()>(not_count/(off_count+not_count))):\n",
    "            predict = \"OFF\"\n",
    "        else:\n",
    "            predict = \"NOT\"\n",
    "        if(val_label[i]==predict):\n",
    "            correct+=1\n",
    "        total+=1\n",
    "acc = correct/total\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Most Frequent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76065625\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for itr in range (0,10000):\n",
    "    temp_zip = list(zip(data, off_label))\n",
    "    random.shuffle(temp_zip)\n",
    "    shuffled_data, shuffled_label = zip(*temp_zip)\n",
    "    div = int(data_size*0.9)\n",
    "    train_data = shuffled_data[0:div]\n",
    "    train_label = shuffled_label[0:div]\n",
    "    val_data = shuffled_data[div:]\n",
    "    val_label = shuffled_label[div:]\n",
    "    off_count = 0\n",
    "    not_count = 0\n",
    "    for i in range(0,len(train_label)):\n",
    "        if(train_label == \"OFF\"):\n",
    "            off_count+=1\n",
    "        else:\n",
    "            not_count+=1\n",
    "    predict = \"\"\n",
    "    if(off_count>not_count):\n",
    "        predict = \"OFF\"\n",
    "    else:\n",
    "        predict = \"NOT\"\n",
    "\n",
    "    for i in range(0,len(val_label)):\n",
    "        if(val_label[i]==predict):\n",
    "            correct+=1\n",
    "        total+=1\n",
    "acc = correct/total\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7640625\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data).toarray()\n",
    "y = [1 if e==\"OFF\" else 0 for e in off_label]\n",
    "correct = 0\n",
    "total = 0\n",
    "for itr in range (0,100):\n",
    "    temp_zip = list(zip(X, y))\n",
    "    random.shuffle(temp_zip)\n",
    "    shuffled_data, shuffled_label = zip(*temp_zip)\n",
    "    div = int(data_size*0.9)\n",
    "    train_data = shuffled_data[0:div]\n",
    "    train_label = shuffled_label[0:div]\n",
    "    val_data = shuffled_data[div:]\n",
    "    val_label = shuffled_label[div:]\n",
    "    off_count = 0\n",
    "    not_count = 0\n",
    "    svm_model = svm.SVC()\n",
    "    svm_model.fit(list(train_data), train_label)\n",
    "    predict = 0\n",
    "    for i in range(0,len(val_data)):\n",
    "        predict = svm_model.predict([val_data[i]])[0]\n",
    "        if(predict==val_label[i]):\n",
    "            correct+=1\n",
    "        total+=1\n",
    "acc = correct/total\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755625\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range = (2,2), token_pattern = r'\\b\\w+\\b', min_df = 1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "X = bigram_vectorizer.fit_transform(data).toarray()\n",
    "y = [1 if e==\"OFF\" else 0 for e in off_label]\n",
    "correct = 0\n",
    "total = 0\n",
    "for itr in range (0,100):\n",
    "    temp_zip = list(zip(X, y))\n",
    "    random.shuffle(temp_zip)\n",
    "    shuffled_data, shuffled_label = zip(*temp_zip)\n",
    "    div = int(data_size*0.9)\n",
    "    train_data = shuffled_data[0:div]\n",
    "    train_label = shuffled_label[0:div]\n",
    "    val_data = shuffled_data[div:]\n",
    "    val_label = shuffled_label[div:]\n",
    "    off_count = 0\n",
    "    not_count = 0\n",
    "    svm_model = svm.SVC()\n",
    "    svm_model.fit(list(train_data), train_label)\n",
    "    predict = 0\n",
    "    for i in range(0,len(val_data)):\n",
    "        predict = svm_model.predict([val_data[i]])[0]\n",
    "        if(predict==val_label[i]):\n",
    "            correct+=1\n",
    "        total+=1\n",
    "acc = correct/total\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Use FastText to vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(tokens[1:]).astype(float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_data = load_vectors(\"wiki-news-300d-1M-subword.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "temp_list = [sentence.split(\" \") for sentence in data]\n",
    "max_len = 0\n",
    "for sentence in temp_list:\n",
    "    if(len(sentence)>=max_len):\n",
    "        max_len = len(sentence)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\JAD/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\JAD\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c6731ecf87fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mexample_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"This is a sample sentence, showing off the stop words filtration.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_sent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# filtered_sentence = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[0;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\JAD/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\JAD\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "  \n",
    "word_tokens = word_tokenize(example_sent) \n",
    "\n",
    "# filtered_sentence = [] \n",
    "  \n",
    "# for w in word_tokens: \n",
    "#     if w not in stop_words: \n",
    "#         filtered_sentence.append(w) \n",
    "print(word_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 1867, in run\n",
      "    for msg in self.data_server.incr_download(self.items):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 529, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 572, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 543, in incr_download\n",
      "    for msg in self.incr_download(info.children, download_dir, force):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 529, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 572, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 549, in incr_download\n",
      "    for msg in self._download_package(info, download_dir, force):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 600, in _download_package\n",
      "    os.remove(filepath)\n",
      "PermissionError: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'C:\\\\Users\\\\JAD\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\framenet_v17.zip'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
